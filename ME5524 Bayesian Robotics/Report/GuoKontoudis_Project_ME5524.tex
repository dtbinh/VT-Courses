\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  

\IEEEoverridecommandlockouts                             
\overrideIEEEmargins

\usepackage{graphics} 
\usepackage{graphicx}
%\usepackage[draft]{graphicx}
\usepackage{todonotes}
\usepackage{epsfig} 
\usepackage{mathptmx} 
\usepackage{times} 
\usepackage{amsmath} 
\usepackage{amssymb}  
\usepackage{makeidx}
\usepackage{float}
\usepackage{balance}
\usepackage{url}
%\usepackage{hyperref}
\usepackage{cite}
\usepackage{flexisym}
\usepackage{chngcntr}
\usepackage{tikz}
\usepackage[hidelinks]{hyperref}

\newtheorem{assumption}{\textbf{Assumption}}
\newtheorem{definition}{\textbf{Definition}}


\DeclareMathOperator*{\maxi}{max}
\DeclareMathOperator*{\mini}{min}

\title{\LARGE \bf Integrated Online Perception of Articulated Objects for Manipulation}

\author{Jia Guo \and George Kontoudis
\thanks{This work was submitted on 07/04/17 and is part of the project for the class ME5524-Bayesian Robotics, offered by Prof. Tomonari Furukawa, \url{tomonari@vt.edu}}
\thanks{J. Guo and G. Kontoudis are with the Mechanical Engineering Department, Virginia Polytechnic Institute and State University, Blacksburg, VA, 24060, USA, \url{jguo18@vt.edu}, \url{gpkont@vt.edu}}% Stops a space
\thanks{Names were alphabetically ordered.}%
}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
An online interactive perception methodology for articulated objects in unstructured environment is presented. The main contribution of this methodology lies in the online solution, which utilize recursive Bayesian estimation techniques. The RGB-D algorithm consist of three sub-recursive estimation problems and each one forms a separate level of estimation. In this way, we can get uncomplicated solution at each level which feed forward and backward information to guarantee robustness, accuracy, and uncertainty elimination. The efficacy of the proposed method is verified through robot manipulation experiments.     
\end{abstract}

\normalsize{\bf\small\emph{Index Terms:} Online perception, recursive Bayesian estimation, manipulation}  

\section{Introduction}\label{intro}
An online mutli-level interactive perception algorithm is presented \cite{martin2014online}, \cite{martin2016integrated}. Grasping and manipulation in unstructured environments require knowledge of the object's shape, position, orientation, and kinematic structure. Visual perception could be a solution to successfully get the information needed for robotic manipulation. Although, it becomes non-trivial to online address this problem and estimate robust solutions. However, the allocation to sub-level algorithms which are interconnected, simplify the overall solution.

In this project we deal with online interactive perception algorithm for robotic manipulation purposes. The algorithm includes the identification of the object's shape, the recognition of its kinematic structure, and the motion tracking of its position and orientation. Moreover, the derivation of the explored object's shape, position, orientation, along with the kinematic structure is achieved. The object's position, orientation, and estimation of kinematic structure part incorporates three recursive Bayesian estimation steps. Then, the shape reconstruction of the investigated object is addressed that thrust the motion tracking.

\begin{figure}[!h]
        \includegraphics[width=.5\textwidth]{figures/Cover.png}
        \centering
        \caption{Integrated online perception applications, a. Drawer opening using RGB-D stream, b. Door opening using RGB-D stream, c. Combination of movements using RGB-D stream, d. Book opening using bag file.}
        \label{cover}
\end{figure}

\par Literature review - Jia
\cite{wuthrich2013probabilistic}, \cite{ochs2014segmentation}, \cite{felzenszwalb2004efficient}, \cite{papon2013voxel}, \cite{chien2002efficient}, \cite{kenney2009interactive}, \cite{stuckler2013efficient}, \cite{ochs2014segmentation}, \cite{martin2014online}, \cite{ross2008unsupervised}, \cite{urgen2009learning}, \cite{sturm2010vision}, \cite{huang2012occlusion}, \cite{katz2014interactive}, \cite{katz2013interactive}.

The remainder of this report is organized as follows. Section \ref{pf} discusses the problem formulation. 
The efficacy of the proposed methodology is assessed in Section \ref{experim} by a set of experiments. Finally, Section \ref{concl} provides the conclusions of the studied methodology and gives directions for future work.  

\section{Problem Formaulation}\label{pf}
In this section we formulate the proposed methodology for integrated online perception. First, the interactive perception problem tracks the motion, and estimates the kinematic structure of the object. Then, this information is provided to the tracker which performs feautre-based tracking, and shape based tracking along with information given from shape-based reconstruction module. The motion segmentation in color and in depth follows to identify fault information from the environment. Next, the object's reconstruction takes place by separating the unreliable information from the picture. Finally, shape-based segmentation is accomplished to feed the shape-based tracking. In figure \ref{RE} the kinematic structure of the methodology is depicted. 

\begin{figure}[!h]
	\includegraphics[scale=0.70]{figures/RecursiveEstimation.pdf}
	\centering
	\caption{Kinematic identification scheme.}
	\label{RE}
\end{figure}

The basic process of the integrated perception method is shown in figure \ref{IP}. The foundation of the method is the multi-level kinematic estimation method. The shape-based tracker is introduced to refine the results of feature-based tracker. As stated in the related work, the visual pose estimation technique is approached in two ways, which are based on shape model and texture feature respectively. Here these two methods are integrated with the process of shape reconstruction. 

\begin{figure}[!h]
	\includegraphics[scale=0.315]{figures/IP_method.png}
	\centering
	\caption{Flow chart of integrated perception method \cite{martin2016integrated}.}
	\label{IP}
\end{figure}

\subsection{Feature Motion Estimation}\label{fme}
The gathered information from an RGB-D camera employed to track feature motion with recursive estimation. The recursive estimation seeks to update the belief of target $p(x_k^t|z_{1:k}^t)$. In this case the targets are features of the object in the $3D$ space, $x_k^f \in \mathbb{R}^{3m}$, where $k \in \mathbb{N}$ is the time index, $f$ is the feature motion, and $m \in \mathbb{N}$ is the number of features we track. Then we get observations from the camera in the 2-$D$ image plane, $z_k^f \in \mathbb{R}^{2m}$, where $k \in \mathbb{N}$ is the time index, $f$ is the feature motion, and $m \in \mathbb{N}$ is the number of features we track. 

The recursive estimation is based on priors of Kanade-Lucas-Tomasi (KLT) tracking algorithm \cite{tomasi1991detection}, \cite{lucas1981iterative} the small motion, and the brightness constancy constraint. Small motion means that the feature points do not move far away from their previous position. Brightness constancy means that the projection of the same feature is the same at each frame. 
%The brightness constancy equation yields
%\begin{equation}
%I(x,y,t)=I(x+u,y+v,t+1)
%\end{equation}
First, a feature initialization takes place from the image, based on Kanade-Tomasi corner detection. Then, a prediction of 3-$D$ feature's location is occurred from its current location and the velocities obtained from the higher stage. Next, this prediction is employed to project this features in the image plane and use them as input to KLT algorithm. 

\subsection{Extended Kalman Filter of Rigid Bodies}\label{EKFrb}
In the second stage an extended Kalman filter (EKF) is utilized to estimate the motion of rigid bodies. The information needed is collected form the lower stage feature tracker and from the upper stage kinematic model estimator. The methodology deals with EKF, because it is based on non-linear Gaussian models. The stochastic motion and sensor models have the form of
\begin{equation}
x_k^t=f^t(x_{k-1}^t,u_{k-1}^t)+w_{k-1}^t,
\end{equation} 
\begin{equation}\label{obsM}
z_{k-1}^t=h^t(x_{k-1}^t)+v_{k-1}^t,
\end{equation} 
where $k \in \mathbb{N}$ is the time index, $x_k^t$ is the vector of states, $z_{k-1}^t$ is the vector of observations, $w_{k-1}$ and $v_{k-1}$ are the zero-mean independent identically distributed Gaussian noises $w_{k-1}^t \sim \mathcal{N}(0, \Sigma_{w_{k-1}^t})=\mathcal{N}(0, Q_{k-1})$, with $Q_{k-1} \geq 0$, and $v_{k-1}^t \sim \mathcal{N}(0, \Sigma_{v_{k-1}^t})=\mathcal{N}(0, R_{k-1})$ with $R_{k-1}>0$.
To estimate the state, the EKF first predicts the state and the covariance as
\begin{equation}
\bar{x}_{k|k-1}^t=f^t(x^t_{k-1|k-1},u^t_{k-1}),
\end{equation}
\begin{equation}
\Sigma_{x_{k|k-1}^t}=A_{k-1} \Sigma_{x_{k-1|k-1}^t} A_{k-1}^{\intercal} +Q_{k-1},
\end{equation}
where
\begin{equation}
A_{k-1}= \frac{\partial f^t(x_{k-1|k-1}^t,u_{k-1})}{\partial x_{k-1|k-1}^t}.
\end{equation}
Then the Kalman gain is calculated from 
\begin{equation}
K_{k}= \Sigma_{x_{k|k-1}^t} C_k^{\intercal}(C_k \Sigma_{x_{k|k-1}^t} C_k^{\intercal}+ R_{k})^{-1},
\end{equation}
where
\begin{equation}
C_k= \frac{\partial h^t(x_{k|k-1}^t)}{\partial x^t_{k|k-1}}.
\end{equation}
Next, the correction is given as
\begin{equation}
\bar{x}_{k|k}^t=\bar{x}_{k|k-1}^t+K_k(z_k^t-h^t(\bar{x}_{k|k-1})),
\end{equation}
\begin{equation}
\Sigma_{x_{k|k}^t}=(I-K_kC_k)\Sigma_{x_{k|k-1}^t}.
\end{equation}

More specifically the correct features from the lower stage are defined as rigid bodies. Every rigid body has its own EKF and the state composed from position and orientation, and twist, $x_k^t = [ p^{\intercal} \hspace{.2cm} \nu^{\intercal} ] ^{\intercal}$, $x_k^t \in \mathbb{R}^{12}$. The observations contain the location of each rigid body in the 3-$D$ space, $z_k^t \in \mathbb{R}^{3m}$. Three simultaneous actions were employed to predict the state of rigid bodies. One action predicts the next state of rigid bodies by employing an EKF as previously described. The other action examines the sudden immobilization of rigid bodies, while the last action employs data from the upper kinematic stage to predict an alternative state of rigid bodies. The observation part includes feature locations in 3-$D$ space $f_0^m$ and the homogeneous transformation using the position and orientation of the rigid bodies $T(p)$. The exact probabilistic sensor model of \ref{obsM} becomes
\begin{equation}\label{obsMF}
z_k^t=h^t(x_{k-1}^t)+v_{k-1}^t= \begin{bmatrix}
T(p)f^1_{0} \\
\vdots \\
T(p)f^m_{0}
\end{bmatrix} + v_{k-1}^t,
\end{equation}

To effectively estimate the motion of a group with multiple rigid bodies the generation of valid sets is needed. After defining these sets the next step is to estimate their motion in 3-$D$ space. For this purpose we employ EKF to predict the group motion, but in case that the error exceeds $2cm$, random sample consensus (RANSAC) is utilized. If the group consists of at least $15$ features we consider it as a valid set.



\subsection{Extended Kalman Filter of Kinematic Model}\label{EKFkm}
An estimation and tracking of the kinematic model is achieved in the third stage. The information needed is gathered from the lower stage rigid body estimator. The kinematic model can be related either as a prismatic joint, or as a revolute joint, or as a rigid connection, or unrelated. Each relation is represented by a different state $x_{joint}^t$ and the observations are collected form the previous stage as a set of multiple rigid bodies $z_{joint}^t \in \mathbb{R}^6$, along with the zero-mean independent identical distributed Gaussian noise $v_{joint}^t$. Since, the type of joint varies the utilization of different observation model was imposed to apply EKF. 

For the prismatic joint estimation the state includes the axis orientation, the joint displacement $q_p \in \mathbb{R}$, and the joint velocity. Joint velocities update by employing the previous stage twist vector. For the position and orientation prediction of the rigid bodies relation, the following observation model was utilized
\begin{equation}
z_{pr, joint}^t=\begin{bmatrix}
q_p \hat{o}_p\\
0_3 
\end{bmatrix} + v_{k-1}^t,
\end{equation}
where $\hat{o}_p \in \mathbb{R}^3$ is the orientation of the axis.

For the revolute joint estimation the state includes the axis orientation, the joint revolution $q_r \in \mathbb{R}$, and the joint velocity. Joint velocities update by employing the previous stage twist vector. For the position and orientation prediction of the rigid bodies relation, the following observation model was utilized
\begin{equation}
z_{rev, joint}^t=\begin{bmatrix}
(-q_r \hat{o}_r) \times p_r\\
q_r \hat{o}_r
\end{bmatrix} + v_{k-1}^t,
\end{equation}
where $\hat{o}_p \in \mathbb{R}^3$ is the orientation of the axis, and $p_r \in \mathbb{R}^3$ is a point on the axis of rotation.

The observation for rigid boy estimation has no information about rigid body relations $z_{rig, joint}^t = 0_6 + v_{k-1}^t$. In case none of the above stands then the algorithm defines unrelated rigid bodies.

\subsection{Feature-Based Tracker}\label{FBT}
Jia

\subsection{Motion Segmentation}\label{MS}
Jia

\subsection{Shape Reconstruction and Shape-Based Segmentation}\label{SR_SBS}
Jia

\section{Experiments and Results}\label{experim}
In this section we perform experiments to validate the efficacy of this project. We intended to implement experiments on grasping, but eventually the robot arm faced some malfunctions and we could not operate it. So we worked on identifying the problem and finding a solution for the robot arm. The first part of the experiments test with bag files the functionality of the ROS \cite{ROS} package OMIP, that was developed from the authors of \cite{martin2014online}, \cite{martin2016integrated}. More information about OMIP can be found on the following link.

\begin{center}
\url{http://wiki.ros.org/omip}
\end{center}

In the next part we employ a RGB-D camera, ASUS Xiton Pro live \cite{AsusXiton} to successfully identify the kinematic structure of a drawer and a door. A video of the conducted experiments can be found in the following link.

\begin{center}
\url{https://youtu.be/weG94fqyQpY}
\end{center}


\subsection{Online Perception Algorithm}\label{ORP}
Bag files\footnote{A bag in ROS is a predefined ROS message data format file.}, provided by the authors of the OMIP package, utilized to identify the kinematic structure of objects. The whole procedure in ROS is depicted in figure \ref{rqt}. This graph consists of the camera, the feature tracker, the rigid body tracker, the joint tracker, and the camera base link. First, the camera base link nodes publish messages on the topic of transformations its initial configuration. These transformation messages are subscribed by the camera manager node and the rigid body tracker node. Then, the camera manager node publishes messages to specific topics that can be efficiently subscribed by the feature tracker. This part is the RGB-D image streaming of the camera as described in section \ref{pf}. Next, the feature tracker node publishes messages to the state topic which are subscribed from the rigid body node, while it subscribes messages from the predicted measurement topic which are published from the rigid body tracker node. This step was described previously as the feature tracker level, where pose feature information fed the next level of rigid body motion and velocity of features were received as feedback simultaneously. Rigid body tracker node publishes messages to the state topic which are subscribed from the joint tracker node, while it subscribes messages from the predicted measurement topic that are subscribed from the joint tracker node. This sector works similar with the rigid body motion level where the next kinematic level receives information regarding the rigid body motion and provides feedback for the kinematic level. The graph architecture is similar with the kinematic identification scheme in figure \ref{RE}. 


\begin{figure}[!h]
        \includegraphics[width=.555\textwidth]{figures/rqt_graph.png}
        \centering
        \caption{Online perception algorithm (OMIP) in ROS using rqt\_graph.}
        \label{rqt}
\end{figure}

\subsection{Experiment Configuration and Results}\label{EV}
Jia

\subsubsection{Book Demo File Test}
Jia


\subsubsection{Drawer Identification Test}
Jia

\subsubsection{Door Identification Test}
Jia

\subsubsection{Mixed Motion Identification Test}
Jia


\subsection{Grasping Application}
We intended to implement a third set of grasping experiments with a robot arm. Although, we had already became familiar with the robot arm platform in simulations we could not work on it, because it was malfunctioned. More specifically, the problem was that the robot left for more than $3$ months on the operating mode and as a result the batteries were completely discharged. Therefore, we identify the problem and we decided to continue working on implementing the online perception technique with an online RGB-D stream from a camera.


\section{Conclusions}\label{concl}
In this project we first study an online perception methodology for kinematic structure identification of articulated objects using a single RGB-D camera. The proposed technique included three sub-level recursive estimation models that made the algorithm efficient enough to operate online. Extended Kalman filter was used as a recursive Bayesian estimation technique because the motion  and sensors models were non-linear. Next, a shape-based tracker employed to refine the outcomes of the feature based tracker. Another recursive, estimation technique that utilized was Kanade-Lucas-Tomasi algorithm for the initial tracking part. Robot arm malfunction and time restriction could not let us work on the implementation of the online perception algorithm with grasping experiments. As a result we implement the perception with bag files and we conducted experiments with an online RGB-D camera. All the experiments resulted the successful identification of the kinematic structure for both the prismatic and the revolute joints. The online perception technique efficiently recognize the kinematic structure of some articulated objects, but needs further development to make it compatible with our robotic systems. The efficacy of the perception method depends mostly on the feature tracking, so we needed to add AR tags even with the shape reconstruction extension.  

As a future work we keen on continuing the software development in order to make the online perception technique compatible with our robots. Moreover, we suggest the utilization of contemporary tracking algorithm for the feature tracking part. Unscented Kalman filter might be an alternative solution for some objects because of its ability to handle heavily non-linear models. Implementation of this combinatorial system might be useful for future research purposes in grasping and manipulation.

\bibliographystyle{IEEEtrans}
\bibliography{IEEEabrv,mybib}

\end{document}